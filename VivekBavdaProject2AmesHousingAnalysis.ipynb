{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8f549b",
   "metadata": {},
   "source": [
    "# Vivek Bavda:\n",
    "## DSIR 7/12\n",
    "### Project 2 August 9, 2021 \n",
    "### Fair Housing: Predicting Home Sale Prices in Ames, Iowa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70608d9",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "*As you are aware Ms. Mayor of Ames, Iowa, there have been concerns about redlining, restrictive covenants, and discrimination by homesellers in our city toward traditionally disadvantaged groups in our country. Given the uproar and your desire to integrate the community, you've asked me to provide a gender and color neutral model of housing prices to determine if there has been a premium required for people of color on homes bought in Ames, Iowa.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17242a26",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Background](#Background)\n",
    "- [Outside Research](#Outside-Research)\n",
    "- [Ames Housing Data Sets](#Ames-Housing-Data-Sets)\n",
    "- [Data Import & Cleaning](#Data-Import-and-Cleaning)\n",
    "- [Exploratory Data Analysis and Data Visualizations](#Exploratory-Data-Analysis-and-Data-Visualizations)\n",
    "- [Modeling](#Modeling)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63000df0",
   "metadata": {},
   "source": [
    "## [Background](#Background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43102e4",
   "metadata": {},
   "source": [
    "The City of Ames, Iowa is committed toward equity in housing. The Mayor has made fair housing a centerpiece of his political campaign and is committed to making the goals of Title VI of the Civil Rights Act of 1964 a reality.   \n",
    "\n",
    "As stated on the City of Ames website, the city will “'Affirmatively further fair housing', which means that the City actively works to reduce illegal housing discrimination. The City promotes equal housing opportunity through education and training, monitoring and investigating fair housing complaints utilizing techniques to support fair housing litigation, and conduct research and studies to identify and address fair housing impediments.[*source*](https://www.cityofames.org/government/departments-divisions-a-h/housing/fair-housing) To be clear Title VI and \"the Fair Housing Act prohibit housing discrimination based on race, color, national origin, religion, handicap, sex, familial status (including children under the age of 18 living with parents or legal custodians; pregnant women and people securing custody of their children under 18).[*source*](https://www.cityofames.org/government/departments-divisions-a-h/housing/fair-housing)\n",
    "\n",
    "As stated above, the city will conduct studies to meet this end, and Bavda Consulting was hired to build a predictive model on housing sale prices to see whether there was discrimination in transactions. Through Multivariate Linear Regression, this data analysis seeks to predict housing prices that are gender and color neutral based on inputs such as square feet, the number of bathrooms and other concrete variables that should determine housing prices. The government has a dataset deemed Ames Training Dataset, which was used to create this model. After the model is built and presented, the Mayor will use the model on a dataset deemed Ames Testing Dataset whose purchases were are unknown to Bavda Consulting. The model will be tested on these transactions to gauge its predictive power. While there is always irreducable error to some extent, the Mayor has set a statistical benchmark goal of 32,000 or less in root mean squared error. This is based on other submissions from his interns and volunteers who scored from 35,000 to the use of the mean as the predictory, which scores appoximately 80,000. This should ensure that the predictions are accurate and will be used to test sellers to see if people of color are forced to pay a premium to purchase homes. This will allow the city focus on the people and places that see the greatest variation in sales price from predicted price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e865e1",
   "metadata": {},
   "source": [
    "## [Outside Research](#Outside-Research)\n",
    "The value of the study is especially important in terms of identifying and rooting out overt and covert discrimination. As John Oliver made clear on his program Last Week Tonight [*source*](https://www.youtube.com/watch?v=_-0J49_9lwc), past overt housing discrimination has lead to the huge gaps in wealth we now know. Most majority Americans have used their homes as an investment vehicle to accumulate wealth. Given that people of color have been left out through outright bans, restrictive covenants(agreements between homeowners to not to sell to people of color, and then redlining, the practice of financial institutions to not lend to people of color for homes in majority communities, this has lead to people of color accumulating far less wealth and essentially segregation through higher prices in wealthier neighborhoods. Even when people of color have the weatlh to purchase homes, they often are forced to pay a premium. This covert discrimination is the target of study. As is mentioned above, the Mayor wants Ames to be different. This analysis is one step in the Mayor's larger plan. Identifying transactions can focus the city and chip away at housing discrimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995a644",
   "metadata": {},
   "source": [
    "## [Ames Housing Datasets](#Ames-Housing-Datasets)\n",
    "#### 2006 - 2010\n",
    "\n",
    "#### The Data Dictionary can be accessed [*here*](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt) at its original source location or within this repository [*here*](./datasets/AmesDataDictionary.docx).\n",
    "\n",
    "* [AmesDatasetTrain]('./datasets/train.csv'): Data set contains information from the Ames, Iowa Assessor’s Office used in computing assessed values for individual residential properties sold in Ames, IA from 2006 to 2010. This dataset has been divided into this train set and validation set by the Mayor for Bavda Consulting to use to create its model. T The data has 81 columns which include 22 nominal, 23 ordinal, 14 discrete, and 20 continuous variables (and 2 additional observation identifiers). \n",
    "\n",
    "\n",
    "* [AmesDatasetTest](./datasets/test.csv):Data set contains information from the Ames, Iowa Assessor’s Office used in computing assessed values for individual residential properties sold in Ames, IA from 2006 to 2010. This dataset has been divided into a train set (above) and a validation set. This validation set includes 80 of 81 columns which include 22 nominal, 23 ordinal, 14 discrete, and 19 continuous variables (and 2 additional observation identifiers). The missing column is Sale Price in this dataset. This data is to be filled by Bavda Consulting using the 81 columns. This ensures and checks that the model developed by Bavda Consulting accurately predicts the price.\n",
    "    \n",
    "    \n",
    "* [AmesDatasetKagle]('./datasets/BavdaFinal.csv'): Data set contains the predictions for the validation set. This dataset has been submitted to the Mayor alongside with this report and model.\n",
    "\n",
    "\n",
    "* Cautionary Note: The amount of observations provided by the Mayor is approximately 2000 observations while there are 82 variables if you include the dependent variable and the identification variables. As a rule of thumb, for every 400 observations, the model to be accurate would need one feature also knows as an independent variable. This suggests that there would be only be 5 features in the model. Given the complexity of the housing market, even without exploratory data analysis, this model probably does not have enough data to answer the problem statement. \n",
    "\n",
    "\n",
    "* Outliers: There are outlandishly expensive for Ames more than 2 standard deviations from the mean up to a $611,657 maximum sale price for one home in this data. They appear to be out of the norm. They have not been removed for two reasons. First, the Mayor has specifically asked us not to exclude data. Second, too often, data scientists remove outliers because they do not match their assumptions about the problem. When these points are removed, the 'usual' model does work. However, the outlier is still real and may occur. When it does, Wall St. analysts, for example, claim 'black swans' far too often, resulting in major losses for their customers. The point is each observation matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefbb973",
   "metadata": {},
   "source": [
    "## [Data Import and Cleaning](#Data-Import-and-Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16e6d1",
   "metadata": {},
   "source": [
    "* This section goes hand and hand with Exploratory Data Analysis. As you can see below, libraries with the tools necessary to complete this work are imported. This includes pandas and sklearn as well as others. Summary Statistics are seen for the dataframes\n",
    "\n",
    "\n",
    "* All of the features and target are viewed. Functions named datacleaning and datacleaningbox were used to look at summary statistics, check histograms, correlation, and plots in order to determine if there is a linear relationship between the feature and target. Data types and null values are checked in these functions. Null values are removed for the relevant variables. The median is imputed for values if the variable has a null value but is too large to ignore. Other rows win null values are small in number are simply dropped.\n",
    "\n",
    "\n",
    "* There is transformation and merging of variables. The relevant features are one-hot coded-otherwise known as dummy variables. There is a comment explaining the reason for inclusion or exclusion into the model. The next section will provide more visuals regarding the included variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa0f5bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "trailing comma not allowed without surrounding parentheses (<ipython-input-1-397f9c2554c9>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-397f9c2554c9>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    from sklearn.model_selection import cross_val_score, cross_validate,\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m trailing comma not allowed without surrounding parentheses\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statistics import mean, stdev\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, Ridge, RidgeCV, LassoCV\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cb7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading Data\n",
    "\n",
    "AmesDatasetTrain='./datasets/train.csv'\n",
    "\n",
    "AmesDatasetTest='./datasets/test.csv'\n",
    "\n",
    "AmesDatasetKagle='./datasets/BavdaFinal.csv'\n",
    "\n",
    "dftrain=pd.read_csv(AmesDatasetTrain)\n",
    "\n",
    "dftest=pd.read_csv(AmesDatasetTest)\n",
    "\n",
    "dfkagle=pd.read_csv(AmesDatasetKagle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain  #Quick first check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42b6ced",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dftrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-615af615aca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdftrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Summary Statistics to start off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dftrain' is not defined"
     ]
    }
   ],
   "source": [
    "dftrain.describe() #Summary Statistics to start off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "dftrain.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e95835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check Data types\n",
    "dftrain.dtypes.sort_values().head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1f3a0",
   "metadata": {},
   "source": [
    "Total values equal 81 variables. Please note above and below that Exterior 1 is repeated. If you count the listings there are 22 varialbes below 60 above. Remove the repeat, an you have 81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72177a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dftrain.dtypes.sort_values().tail(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4caff86",
   "metadata": {},
   "source": [
    "The dependent variable in this analysis is Sale Price. Below is a preliminary analysis. It appears to be normally distributed with a positive skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dftrain['SalePrice'].describe())\n",
    "print(dftrain['SalePrice'].isnull().sum())\n",
    "print(dftrain['SalePrice'].unique())\n",
    "print(dftrain['SalePrice'].value_counts())\n",
    "print(dftrain['SalePrice'].hist())\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109332d",
   "metadata": {},
   "source": [
    "## Functions Used for Data Cleaning and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a852789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For EDA and Data cleaning\n",
    "def datacleaningbox(feature):\n",
    "    print(dftrain[feature].describe())\n",
    "    print(dftrain[feature].isnull().sum())\n",
    "    print(dftrain[feature].unique())\n",
    "    print(dftrain[feature].value_counts())\n",
    "    print(dftest[feature].describe())\n",
    "    print(dftest[feature].isnull().sum())\n",
    "    print(dftest[feature].unique())\n",
    "    print(dftest[feature].value_counts())\n",
    "    print(dftrain[feature].hist())\n",
    "    sns.pairplot(dftrain, x_vars=[feature], y_vars=['SalePrice'])\n",
    "    print(dftrain[[feature, 'SalePrice']].corr())\n",
    "    sns.boxplot(x=dftrain['SalePrice'], y=dftrain[feature])\n",
    "    return\n",
    "\n",
    "def datacleaning(feature):\n",
    "    print(dftrain[feature].describe())\n",
    "    print(dftrain[feature].isnull().sum())\n",
    "    print(dftrain[feature].unique())\n",
    "    print(dftrain[feature].value_counts())\n",
    "    print(dftest[feature].describe())\n",
    "    print(dftest[feature].isnull().sum())\n",
    "    print(dftest[feature].unique())\n",
    "    print(dftest[feature].value_counts())\n",
    "    print(dftrain[feature].hist())\n",
    "    sns.pairplot(dftrain, x_vars=[feature], y_vars=['SalePrice'])\n",
    "    print(dftrain[[feature, 'SalePrice']].corr())\n",
    "    return\n",
    "\n",
    "def lasso_coefs(X, y, alphas):\n",
    "    coefs = []\n",
    "    lasso_reg = Lasso()\n",
    "    for a in alphas:\n",
    "        lasso_reg.set_params(alpha=a)\n",
    "        lasso_reg.fit(X, y)\n",
    "        coefs.append(lasso_reg.coef_)\n",
    "        \n",
    "    return coefs\n",
    "\n",
    "def print_results(scores_list):\n",
    "    return f'''\n",
    "    [min, max] = \n",
    "                        {[round(min(scores_list), 2), round(max(scores_list), 2)]}\n",
    "    \n",
    "    confidence interval: \n",
    "                         {round(mean(scores_list), 2)} \\u00B1 {round(2 * stdev(scores_list), 2)}\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab479b",
   "metadata": {},
   "source": [
    "## Each variable has undergone an examination\n",
    "## through the use of above functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36207cc",
   "metadata": {},
   "source": [
    "Variable 1: Id is an index.  It is not in the data dictionary included with the repoository. No relevance to this project other than to identify predictions for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845489eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Id')             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70831a02",
   "metadata": {},
   "source": [
    "'2nd Flr SF' is the second variable examined. There are no nulls. There are 0's.\n",
    "This is probably ranch or single story homes. This was combined with '1st Flr SF'\n",
    "to get an overall 'square_feet' number for each house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a81083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datacleaning('2nd Flr SF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8efe3",
   "metadata": {},
   "source": [
    "'1st Floor SF' contains no nulls nor absurdities such as 0's. It makes sense to combine this variable with 2nd Floor SF to create square_feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datacleaning('1st Flr SF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17956190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execution of above statements\n",
    "dftrain['square_feet'] = dftrain['1st Flr SF'] + dftrain['2nd Flr SF']\n",
    "dftest['square_feet']=dftest['1st Flr SF'] + dftest['2nd Flr SF']\n",
    "datacleaning('square_feet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a6574",
   "metadata": {},
   "source": [
    "'Low Qual Fin SF' is examined. This appears to be a useless variable. There are not enough data points to be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f986fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Low Qual Fin SF')  #drop id, 1stflsq, 2ndflsq, low qual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf5cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Low Qual Fin SF')  #drop id, 1stflsq, 2ndflsq, low qual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cbe204",
   "metadata": {},
   "source": [
    "'Gr Liv Area' is examined. This looks no different than square feet created above. Moreover,square feet's correlation is higher than this one by a small margin.\n",
    "This means we can now drop id, 1stflsq, 2ndflsq, gr liv area and low qual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955aaffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Gr Liv Area')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135dbec",
   "metadata": {},
   "source": [
    "'Full Bath''s .54 correlation suggest predictive power. There may be interaction\n",
    "with square feet. This correlation went up when Half Bath was added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b803b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Full Bath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017835c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Half Bath')  #Add half to Full Bath\n",
    "dftrain['Full Bath']=dftrain['Full Bath'] + ((dftrain['Half Bath'])/2)\n",
    "dftest['Full Bath']=dftest['Full Bath'] + ((dftest['Half Bath'])/2)\n",
    "datacleaning('Full Bath')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8a1de",
   "metadata": {},
   "source": [
    "'Bedroom AbvGr' is examined. The correlation is surprisingly low.\n",
    "This probably wouldn't be included as there are approximately 2000 observations\n",
    ", which suggest we only have 5 independent varibles for use in our regression.\n",
    "\n",
    "List of dropped variables include id, 1stflsq, 2ndflsq, gr liv area, bedroom abvgr and 'Low Qual Fin SF'\n",
    "These should be included: sq feet,half bath and full Bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Bedroom AbvGr')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87bb1d",
   "metadata": {},
   "source": [
    "'Kitchen AbvGr' is examined. Based on low correlation with the target and the scatterplot below, this variable is left out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ad251",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Kitchen AbvGr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef2f3d",
   "metadata": {},
   "source": [
    "'TotRms AbvGrd' is examined. The histogram shows a wide variety of data points, and the correlation is high. There may be colinearity with Square Feet, but the Ridge and Lasso penalties used below will account for this. It is included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('TotRms AbvGrd')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901376f",
   "metadata": {},
   "source": [
    "'Fireplaces' is examined below. There is a .47 correlation. Moreover, the scatterplot shows\n",
    "a distinct linear relationhip.  Therefore, it is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec642856",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Fireplaces') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8ac98",
   "metadata": {},
   "source": [
    "'Open Porch SF' is examined. Due to the many 0's and the lack of a linear relationship in\n",
    "the scatterplot, I have excluded this variable even though the correlation is .33. Moreover, we have only 2000 observations, which suggests a 5 variable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e890ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Open Porch SF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e61b84",
   "metadata": {},
   "source": [
    "'Enclosed Porch' is examined below. This can be dropped. There are 1700 0 values with a low correlation. With a likely 5 variable regression model, this is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216a0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Enclosed Porch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd08fb",
   "metadata": {},
   "source": [
    "'3Ssn Porch' is examined below. This can be dropped. There are 2025 0 values with a low correlation. With a likely 5 variable regression model, this is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('3Ssn Porch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a587a",
   "metadata": {},
   "source": [
    "'Pool Area' is examined below. This can be dropped. There are 2042 0 values with a \n",
    "low correlation. With a likely 5 variable regression model, this is dropped. However, these houses are probably outliers that will lower R2 for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Pool Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feada1a6",
   "metadata": {},
   "source": [
    "'Misc Val' is examined below. This can be dropped. There are 1986 0 values with a low correlation. With a likely 5 variable regression model, this is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf4ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Misc Val') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98589ffc",
   "metadata": {},
   "source": [
    "'Mo Sold' should have a value as more homes are sold in the summer than winter. However, it is impossible to tell in its current format. This variable was dummied to see if there was a high correlation with Sale Price. This was not seen for any month in the heatmap that is created if you scroll further down. It appears that there may be more homes sold in summer, but it does not effect price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f62faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Mo Sold')\n",
    "dftrain=pd.get_dummies(dftrain, prefix='Month', columns=['Mo Sold'], drop_first=True)\n",
    "dftest=pd.get_dummies(dftest, prefix='Month', columns=['Mo Sold'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630700fe",
   "metadata": {},
   "source": [
    "'Yr Sold' was examined. My thought was this should have an effect as the housing crisis and financial panic hit homeowners harshly in 2009 and 2010. I converted these to dummy variables for 2006-2010. The correlations were really low for the dummies, suggesting Ames was not as hard hit. This was excluded from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Yr Sold')\n",
    "dftrain=pd.get_dummies(dftrain, prefix='Year', columns=['Yr Sold'], drop_first=True)\n",
    "dftest=pd.get_dummies(dftest, prefix='Year', columns=['Yr Sold'], drop_first=True)\n",
    "datacleaning('Year_2009')\n",
    "datacleaning('Year_2010')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23561f",
   "metadata": {},
   "source": [
    "'Wood Deck SF' is examined. Correlation is .32 with Sale Price. Even with many homes without a deck, this may be indicative of a higher price. It is worth letting ridge and lasso take a look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a6e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Wood Deck SF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa52d79",
   "metadata": {},
   "source": [
    "'Year Remod/Add' is examined. The correlation appears to be .55.  If there was no remodel, the data dictionary says the original construction date was used. So this confuses what this variable represents. Even homes with homes that are remodeled, older homes still tend to have more issues than newer homes. Year Built probably has more explanatory power(.57), and this definitely will created colinearity with Year Built. This one is excluded as ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Year Remod/Add')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f4752",
   "metadata": {},
   "source": [
    "'Lot area' is examined. Generally speaking, this should have an effect but correlation is .29. The graph doesn't appear as though it has explanatory power. It is put in to let lasso and ridge determine its importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce31c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Lot Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8747efc",
   "metadata": {},
   "source": [
    "'PID'  is simply identification. Any correlation is coincidental. This is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb39427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('PID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1a077",
   "metadata": {},
   "source": [
    "'MS Subclass' is examined. These are divided into types of homes, a nominal variable. I went ahead and dummied the variables to see if these classes made sense. The only one that had a high correlation was MS Subclass_60 which is included in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a130bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('MS SubClass')\n",
    "dftrain=pd.get_dummies(dftrain, prefix='MS SubClass', columns=['MS SubClass'], drop_first=True)\n",
    "dftest=pd.get_dummies(dftest, prefix='MS SubClass', columns=['MS SubClass'], drop_first=True)\n",
    "datacleaning('MS SubClass_60')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae9c6c",
   "metadata": {},
   "source": [
    "'Overall Qual' should be included  based on the graphs and high correlation. This appears to rate the homes on a 1-10 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Overall Qual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91848bbb",
   "metadata": {},
   "source": [
    "'Year Built' appears to have an effect at a correllation of .57. This essentially tells us the age of the home. As stated above, this is a better variable than year remodeled, which also names many values before in that variable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Year Built')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2c1ea",
   "metadata": {},
   "source": [
    "'Overall Cond' is examined. While this should have an effect, the  correllation is low. The graph conflicts with the correllation.  Given the number of variables already included, this is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Overall Cond')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c79318",
   "metadata": {},
   "source": [
    "'Total Bsmt SF' is examined. It shows a strong correllation in its number and on the graph. There is one Nan. This data point is removed. DFTest has no Nans.  The variable is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.dropna(subset=['Total Bsmt SF'], inplace=True)\n",
    "datacleaning('Total Bsmt SF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56232745",
   "metadata": {},
   "source": [
    "'BsmtFin SF 2' is examined below. It contains one Nan that could be dropped if necessary. However, correlation is low, and the graph does not show a relationship. This is not included.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('BsmtFin SF 2')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d813823",
   "metadata": {},
   "source": [
    "'Garage Cars' has high correlation even though the graph does not show a linear relationship.  While it was initially included, there is better garage variable, Garage Area, that is included. There was one NAN, which was dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67fa400",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.dropna(subset=['Garage Cars'], inplace=True)\n",
    "datacleaning('Garage Cars') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f61a16",
   "metadata": {},
   "source": [
    "'BsmtFin SF 1' is examined. This variable has colinearity with 'Total Bsmt SF.' The latter is a better variable as it covers a wider swath of variation.  Below the table compares the two variables with Sale Price. The latter is better. Moreover, given the 2000 observations, this may be a 5 variable model. A multiplication of the two variables for feature interaction doesnot seem useful in this context. 'Total Bsmt SF' is kept while the other is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ac54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dftrain[['BsmtFin SF 1', 'Total Bsmt SF', 'SalePrice']].corr())\n",
    "datacleaning('BsmtFin SF 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1c1cd",
   "metadata": {},
   "source": [
    "'Bsmt Full Bath' is examined below. This variable has colinearity with 'Total Bsmt SF.' The latter is a better variable as it covers a wider swath of variation.  Below the table compares the two variables with Sale Price. The latter is better. Moreover, given the 2000 observations, this may be a 5 variable model. A multiplication of the two variables for feature interaction doesnot seem useful in this context. 'Total Bsmt SF' is kept while the other is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dftrain[['Bsmt Full Bath', 'Total Bsmt SF', 'SalePrice']].corr())\n",
    "\n",
    "datacleaning('Bsmt Full Bath')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc87234",
   "metadata": {},
   "source": [
    "'Bsmt Unf SF' is examined below. This variable has colinearity with 'Total Bsmt SF.' The latter is a better variable as it covers a wider swath of variation. Below the table compares the two variables with Sale Price. The latter is better. Moreover, given the 2000 observations, this may be a 5 variable model. A multiplication of the two variables for feature interaction doesnot seem useful in this context. 'Total Bsmt SF' is kept while the other is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dftrain[['Bsmt Unf SF', 'Total Bsmt SF', 'SalePrice']].corr()) \n",
    "datacleaning('Bsmt Unf SF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3129e",
   "metadata": {},
   "source": [
    "'Lot Frontage' is examined below. The graph makes it look like a blob, suggesting no linear relationship. Even with a .34 corr, this is left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7357be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Lot Frontage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f902ef2",
   "metadata": {},
   "source": [
    "'Garage Yr Blt' is examined below. This variable has colinearity with 'Year Built' and 'Garage Area.' The latter is a better variable as it covers a wider swath of variation.  Moreover, given the 2000 observations, this may be a 5 variable model. A multiplication of the two variables for feature interaction doesnot seem useful in this context. 'Garage Area' is kept while the other is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b089728",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Garage Yr Blt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c3593",
   "metadata": {},
   "source": [
    "'Mas Vnr Area' and 'Mas Vnr Type' are examined. There were Nans in the former. The median was used to impute the value given the distribution. The mean would artificially skew it. This has a .50 correlation so it is included.\n",
    "'Mas Vnr Type' could not be examined in its present formation so it was dummied. The heatmap indicates its usefulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d56f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain['Mas Vnr Area']=dftrain['Mas Vnr Area'].fillna(dftrain['Mas Vnr Area'].median())\n",
    "dftest['Mas Vnr Area']=dftest['Mas Vnr Area'].fillna(dftest['Mas Vnr Area'].median())\n",
    "datacleaning('Mas Vnr Area')\n",
    "datacleaningbox('Mas Vnr Type')\n",
    "dftrain=pd.get_dummies(dftrain, prefix='Masonry Type', columns=['Mas Vnr Type'], drop_first=True)\n",
    "dftest=pd.get_dummies(dftest, prefix='Masonry Type', columns=['Mas Vnr Type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Bsmt Half Bath')#low correlation-doesn't look useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce42ee1",
   "metadata": {},
   "source": [
    "'Garage Area' is examined. It appears that Garage Area is a better variable than Garage Cars so this one is included. There was no multiplication for feature interaction as this dataset has only 2000 observations, meaning the model will be small.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05978c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Garage Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6acb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaning('Bsmt Unf SF') #low correlation-graph blob-excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5285cbc",
   "metadata": {},
   "source": [
    "'Sale Type' is examined. This was dummied. Given that 1781 observations are WD and that 400 rows per feature is a rule of thumb, this does not strike me as worth including. However, I wanted to see the heatmap after the dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Sale Type') \n",
    "dftrain=pd.get_dummies(dftrain, prefix='Sale Type', columns=['Sale Type'], drop_first=True)\n",
    "dftest=pd.get_dummies(dftest, prefix='Sale Type', columns=['Sale Type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906365b",
   "metadata": {},
   "source": [
    "'House Style' was examined. This was dummied as it is a type. The heatmap indicates its usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('House Style')\n",
    "dftrain=pd.get_dummies(dftrain, prefix='House Style', columns=['House Style'], drop_first=True)\n",
    "dftest=pd.get_dummies(dftest, prefix='House Style', columns=['House Style'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e834979",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Misc Feature')  #Too many missing points, NaN's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6b24b",
   "metadata": {},
   "source": [
    "'Bldg Type' is examine. One category dominates. It does not look as though it makes a difference based on the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Bldg Type')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5902230",
   "metadata": {},
   "source": [
    "Condition 1 is examined. Based on the data dictionary and that the vast numbers of these are normal, I haven't dummied this variable. Again given only about 2000 observations, everything cannot be included without overfitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Condition 1')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7762a6",
   "metadata": {},
   "source": [
    "Condition 2 is examined. Given that 2024 of 2050 are normal, it does not appear to make a difference.  Again too few observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Condition 2') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc0537",
   "metadata": {},
   "source": [
    "Garage Finish is examined. There is already Garage Area includedd. This seems like a minor detail-the number of features already included makes this prohibitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Garage Finish') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d30e52",
   "metadata": {},
   "source": [
    "'Land Contour' is examined. The flatness of the property seems minor and most are the same, 1841. The number of observations make this prohibitive as a feature-Excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee660abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Land Contour') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2d464",
   "metadata": {},
   "source": [
    "'Land Slope' is examined. There are 1951 out of 2049 observations with the same value. This seems prohibitive given the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74303029",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Land Slope') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915b68d",
   "metadata": {},
   "source": [
    "'Utilities' is examined. There are only 2 non all utility observations;it is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d6eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Utilities')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e06181",
   "metadata": {},
   "source": [
    "'Lot Config' is examined. I think this is one to dummy to include. There appears to be variation. The heatmap shall tell the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Lot Config')  \n",
    "dftrain=pd.get_dummies(dftrain, prefix='lot', columns=['Lot Config'], drop_first=True)\n",
    "dftest=pd.get_dummies(dftest, prefix='lot', columns=['Lot Config'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ff11e",
   "metadata": {},
   "source": [
    "'Fence' is examined. We are missing way too much of the data(1651 points). This is not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e19ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Fence') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7217be27",
   "metadata": {},
   "source": [
    "'Lot Shape' is examined. The distribution looks fairly similar to the above variable. Too many are just normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Lot Shape') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Alley')  #Too much missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Street')  #Only 7 aren't paved--excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b318d",
   "metadata": {},
   "source": [
    "'MS Zoning' is examined. Given that 1598 are the same, adding this feature seems to be a luxury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('MS Zoning')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c418a2",
   "metadata": {},
   "source": [
    "'Neighborhood' is examined. \n",
    "This is dummied to see if the neighborhood makes a difference. For one neighborhood, it makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7460c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Neighborhood')  \n",
    "dftrain=pd.get_dummies(dftrain, prefix='neighborhood', columns=['Neighborhood'], drop_first=True)\n",
    "dftest=pd.get_dummies(dftest, prefix='neighborhood', columns=['Neighborhood'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf9ddd",
   "metadata": {},
   "source": [
    "'Roof Style' is examined. One type dominates. It is excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ccc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Roof Style') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef663b7",
   "metadata": {},
   "source": [
    "'Bsmt Qual' is examined. From a common sense persepctive, the height of the basement does not seems to be an explainer in as long as it is within a normal amoount. Here all but 61 are fine. Moreover, basement total square feet is a better explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d63682",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Bsmt Qual')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e90bb7",
   "metadata": {},
   "source": [
    "'Exterior 1st' is examined. This detail does not seem worth including given the number of existing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Exterior 1st') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de95461",
   "metadata": {},
   "source": [
    "'Fireplace Qu' is examined. Most of the values are na or within two categories. excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9dff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Fireplace Qu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Garage Qual') #1831 were typical. Excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1434203",
   "metadata": {},
   "source": [
    "'Functional' is examined. 1914 are typical. Given limited features, I don't think this will help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Functional')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9d882",
   "metadata": {},
   "source": [
    "'Garage Cond' is examined. 1867 are typical. Given limited features, I don't think this will help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Garage Cond')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d844c5",
   "metadata": {},
   "source": [
    "'Kitchen Qual' is examined #Good deal of variability in boxplot. I am transforming into numbers. 1 is low quality. 5 is excellent.\n",
    "#mapping={'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Kitchen Qual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802fe39c",
   "metadata": {},
   "source": [
    "The next 10 code statements change through this mapping={'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2}. This would not work if put in the same cell. This transforms most of the variable for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain==dftrain.replace('Ex', 5, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf47403",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain==dftrain.replace('Gd', 4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37288f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain==dftrain.replace('TA', 3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain==dftrain.replace('Fa', 2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain==dftrain.replace('Po', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70185af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest==dftest.replace('Ex', 5, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest==dftest.replace('Gd', 4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest==dftest.replace('TA', 3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest==dftest.replace('Fa', 2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest==dftest.replace('Po', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dftrain['Kitchen Qual'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211007e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dftest['Kitchen Qual'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dftrain['Kitchen Qual'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05b070",
   "metadata": {},
   "source": [
    "'Paved Drive' is examined. This does not have good intuition, and most are Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Paved Drive') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04556d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Pool QC') #9 observations--exclude as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a1a35",
   "metadata": {},
   "source": [
    "'Electrical' is examined. The fuse box is too granular to make a difference and 1868 are standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc65bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Electrical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fcaa0c",
   "metadata": {},
   "source": [
    "The next 4 code statements change the yes, no statements to 1, 0 to turn into dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain == dftrain.replace('Y', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dff8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain == dftrain.replace('N', 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dftest == dftest.replace('Y', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest == dftest.replace('N', 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef898279",
   "metadata": {},
   "source": [
    "'Central Air' is examined. The vast majority have central air but worth a lasso or ridge attempt. Transformed to 1,0 dummy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Central Air')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ebf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dftrain['Central Air'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad98140",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Roof Matl')  #2023 are one category--excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac47f8f",
   "metadata": {},
   "source": [
    "'Heating QC' is examined. It shows high variation and high correlation. Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Heating QC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa434681",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('BsmtFin Type 2') #very little variation-excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589f908",
   "metadata": {},
   "source": [
    "'BsmtFin Type 1' is examined. While there is gradation, the number of features already is large. Bsmt is included in square feet already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('BsmtFin Type 1') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a76a0",
   "metadata": {},
   "source": [
    "'Bsmt Exposure' is examined. Too detailed for the number of features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Bsmt Cond')#Doesn't show much variability-excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b298113",
   "metadata": {},
   "source": [
    "'Foundation' is examined. This is excluded as this is usually seen at inspection. \n",
    "This is eventually fixed, leading to a buy or no buy rather than price change. Given the number of features already entered, this was excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Foundation')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc9d55",
   "metadata": {},
   "source": [
    "'Exter Cond' is examined. The vast majority are average (1778) so excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Exter Cond') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de306d78",
   "metadata": {},
   "source": [
    "'Exter Qual' is examined. It has a high correlation so included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Exter Qual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc0e66",
   "metadata": {},
   "source": [
    "'Exterior 2nd' is examined and excluded. This detail does not seem worth including given the number of existing features especially when this is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95589782",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Exterior 2nd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3362b01",
   "metadata": {},
   "source": [
    "'Heating is examined.'Virtually all, 2016 are ga. No variation so no help-excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a700410",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Heating')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242f7e1",
   "metadata": {},
   "source": [
    "'Garage Type' is examined and excluded. The type of garage seems less relevant than having a garage that was included in garage sq feet. Moreover, too many other variables have been included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacleaningbox('Garage Type') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3f020",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4b3e0",
   "metadata": {},
   "source": [
    "## [Exploratory Data Analysis and Data Visualizations](#Exploratory-Data-Analysis-and-Data-Visualizations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648eb2a",
   "metadata": {},
   "source": [
    "* The previous section Data Cleaning goes hand in hand with Exploratory Data Analysis. With each variable, correlation between the feature and target was examined. A Box Plot or Pair Plot between the feature and target was created. A histogram of each feature was used to check the distribution of data. Unique values and data types were checked. This is all detailed for every variable above. This section provides a \"Cliff Notes\" visual view of the key variables.\n",
    "\n",
    "\n",
    "* This section shows relevant variables for the modeling analysis. First, summary statistics are provided for the key variables used in the analysis. Below that, you will find a list of variables that have correlation based on R2 of .35 or higher with the target, Sale Price.  Then, there is a features heatmap with higher than .5 R2 correlation. As measured by R2, the absolute value would tell you how strong a linear relationship exists between the feature and target;the closer to 1, the greater the relationahip. \n",
    "\n",
    "\n",
    "* Further down are histograms of the key variables. Distributions are examined and evaluated at an individual level above in comments above the code in the cleaning section. In describing the target, it should be noted that taking the natural log of the target would push the histogram together, allowing for a better predictive process. However, this would also make the model more complicated, making it more difficult to explain. Given the political nature of this project, simpler was considered better.\n",
    "\n",
    "\n",
    "* As you might imagine, the data science process links all of these sections. Using the Lasso Model, several coefficients were eliminated from the final list of variables. This included Masonry Type_None, Central Air, Open Porch SF, Total Rms Abv Grd, and MS Subclass 60 were eliminated due to multicollinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bed1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.describe()[['SalePrice','Garage Area','Total Bsmt SF', 'Overall Qual','Full Bath',\\\n",
    "                    'square_feet', 'neighborhood_NridgHt', 'Fireplaces']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ab6db",
   "metadata": {},
   "source": [
    "As discussed in the summary above, one observation to note about the dependent variable, SalePrice, and demonstrated on the histogram below is the positive skew. The maximum home sale price is 611,657 dollars whereas the 75 percentile is 214,000 dollars. This suggests that there may be outliers that can distort the \"usual\" prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a79d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.describe()[['BsmtFin SF 1','Kitchen Qual', 'Heating QC', 'Exter Qual',\n",
    "                    'Wood Deck SF','Year Built', 'Lot Area', 'Mas Vnr Area', 'Sale Type_New']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0783ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.corr()[['SalePrice']][(dftrain.corr()['SalePrice'].sort_values().abs() > .35)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae58dd7",
   "metadata": {},
   "source": [
    "The above list shows high correlation variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dftrain, x_vars=['Garage Area','Total Bsmt SF', 'Overall Qual','Full Bath','square_feet', \\\n",
    "         ], y_vars=['SalePrice']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca13d4a",
   "metadata": {},
   "source": [
    "The graphs show a linear relationship between the features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dftrain, x_vars=['Fireplaces', 'BsmtFin SF 1', 'Kitchen Qual', 'Heating QC', 'Exter Qual' \n",
    "                              ], y_vars=['SalePrice']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b26dbd",
   "metadata": {},
   "source": [
    "The graphs show a linear relationship between the features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aebee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dftrain, x_vars=['Exter Qual', 'Wood Deck SF',\n",
    "          'Year Built', 'Lot Area', 'Mas Vnr Area'], y_vars=['SalePrice']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c056938f",
   "metadata": {},
   "source": [
    "The graphs show a linear relationship between the features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e273e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.heatmap(dftrain.corr()[['SalePrice']][(dftrain.corr()['SalePrice'].sort_values().abs() > .5)], \\\n",
    "            annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set(title = \"Correlation with Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ea73b",
   "metadata": {},
   "source": [
    "The heatmap above shows features with .5 R2 correlation score. \n",
    "These variables individually show have a strong linear relationship.\n",
    "However, they may correlate with each other, violating assumption 6 \n",
    "of multivariate regression, independence of features. While this is \n",
    "a cause for concern, the Lasso and Ridge Modeling has filtered out the variables to minimize this concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain[['SalePrice','Garage Area','Total Bsmt SF', 'Overall Qual','Full Bath',\\\n",
    "        'square_feet', 'neighborhood_NridgHt', 'Fireplaces', 'BsmtFin SF 1',\\\n",
    "        'Kitchen Qual', 'Heating QC', 'Exter Qual','Wood Deck SF','Year Built',\n",
    "         'Lot Area', 'Mas Vnr Area', 'Sale Type_New']].hist(figsize=(13,13));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f964b9fc",
   "metadata": {},
   "source": [
    "The variables above are all significant in adding explanatory power to the predicted price. Each, in its dimension, helps create the line to tie the data points together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449cb4b7",
   "metadata": {},
   "source": [
    "## [Modeling](#Modeling)\n",
    "\n",
    "###  Caution and Methodology\n",
    "\n",
    "* To summarize the data cleaning and exploratory data analysis, it appears that there will be a workable model to help the Mayor. However, the small amount of observations used in this modeling based on the dataset provided means that we will not be able to hit the predictive levels that data science has reached in many medical fields. A rule of thumb is that for every 400 observations, one feature can be added.  Here, we are using more than that. If there were more data, this analysis would be much better, and the predictive power of the model would be stronger.\n",
    "\n",
    "\n",
    "* Three models, mulitvariate linear regression,  Lasso Penalty Multivariate Linear Regression, and Ridge Penalty Multivariate Linear Regression were used to examine the relationship between the features and the target variable. Even though the Mayor has set his 35,000 root mean square baseline that his intern achieved to compare this model to, generally, these models on the most basic level are compared to the using the average for SalePrice as a predictor for all and compare it to the model.\n",
    "\n",
    "\n",
    "* While the Mayor has his validation set to check the model that is scored by Kaggle, this analysis also used the train, test, and split and cross validation methods of resampling data. This was done to mimic new data being provided to the model prior to Kaggle scoring. The data was divided into fifths. 80 percent was used to 'teach' the model, and the remaining 20 percent was used to generalize a prediction to this 'test' data. This 'teaching and 'testing' was then rotated to check the prowess of the model. This methodology was used for all three models. \n",
    "\n",
    "\n",
    "* In analyzing the value of these models, it is important to use metrics beyond the Mayor's validation set. Certainly, root mean square error (RMSE), the metric that the Mayor will evaluate us on is given. The closer to 0 it is, the better. However, the magnitude of y, the dependent variable, changes this score. Therefore, one can't compare RMSE of a model predicting sales of GM and with a mom and pop gum manufacturer. Perhaps better, the Coefficient of Determination, r squared(R2), is a popular metric. An R2 value is the  percentage of variability in the target that is explained by the independent variables in the model. This can be compared across models. This R2 is used for the cross validation and train test split as described avove. The R2 used for the traditional baseline\n",
    "\n",
    "\n",
    "* A key difference in how the data was treated is that the Lasso model and Ridge model were scaled first to prevent the magnitude of the data from distorting the result. This is not necessary for ordinary multivariate regression models. The next key distinction for these models is regularization.\n",
    "\n",
    "\n",
    "* As Kiefer Katovich of General Assembly in his 'Introduction to Regularization' states, \"the goal of 'regularizing' regression models is to structurally prevent overfitting by imposing a penalty on the coefficients of the model. Regularization methods like the Ridge and Lasso add this additional \"penalty\" on the size of coefficients to the loss function. When the loss function is minimized, this additional component is added to the residual sum of squares. In other words, the minimization becomes a balance between the error between predictions and true values and the size of the coefficients.' In laymen's term, Lasso and Ridge attempt to simplify and streamline the model.\n",
    "\n",
    "\n",
    "### Four Runs Made\n",
    "* The initial run of features included: Garage Cars,  Total Bsmt SF, Overall Qual,  Full Bath, and square_feet resulting in a Kaggle Score of 36800.\n",
    "\n",
    "\n",
    "* The second run with all three models running included the following features:Garage Area, Total Bsmt SF, Overall Qual, Full Bath, square_feet, MS SubClass_60, neighborhood_NridgHt, Fireplaces, TotRms AbvGrd, BsmtFin SF 1, Kitchen Qual, Heating QC, and Exter Qual.  This resulted in a low Kaggle Score of 32686.\n",
    "\n",
    "\n",
    "* The third run with all three models included the following features:Garage Area, Total Bsmt SF, Overall Qual, Full Bath, square_feet, MS SubClass_60, neighborhood_NridgHt, Fireplaces, TotRms AbvGrd, BsmtFin SF 1, Kitchen Qual, Heating QC, Exter Qual, Wood Deck SF, Open Porch SF, Year Built, Lot Area, Mas Vnr Area, Central Air, Masonry Type_None, and Sale Type_New. The best Kaggle RMSE Score was 31850.\n",
    "\n",
    "\n",
    "* The fourth run used Lasso to remove variables. Lasso suggested that Masonry Type_None, Central Air, Open Porch Sf, total Rms Abv Grd, and MS Subclass 60 be removed from the model. This last run was left with the following features :Garage Area,Total Bsmt SF, Overall Qual, Full Bath, square_feet, neighborhood_NridgHt, Fireplaces, BsmtFin SF 1, Kitchen Qual, Heating QC, Exter Qual, Wood Deck SF, Year Built, Lot Area, Mas Vnr Area, and Sale Type_New. This resulted in a RMSE Kaggle Score of 31502.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe3059",
   "metadata": {},
   "source": [
    "### Ordinary Multivariate Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60508e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define features(X) and target\n",
    "features=['Garage Area','Total Bsmt SF', 'Overall Qual','Full Bath','square_feet', \\\n",
    "         'neighborhood_NridgHt', 'Fireplaces','BsmtFin SF 1', 'Kitchen Qual', 'Heating QC',\\ 'Exter Qual', 'Wood Deck SF',\\\n",
    "          'Year Built', 'Lot Area', 'Mas Vnr Area', 'Sale Type_New']\n",
    "\n",
    "#Features\n",
    "X=dftrain[features]\n",
    "target=dftrain['SalePrice']\n",
    "\n",
    "#y is the target\n",
    "y = target \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8da37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d28f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the resampling of data discussed above\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=.2, random_state=41)\n",
    "\n",
    "#verifying  dimensions\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390285a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the estimator\n",
    "lr = LinearRegression(n_jobs=-1)\n",
    "\n",
    "#fit estimator\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#predict to score\n",
    "odd=lr.predict(X_test)\n",
    "\n",
    "# The intercept and Coefficients\n",
    "print('Coefficient values of the Model are ', lr.coef_)\n",
    "print('Intercept value is ', lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315eaae",
   "metadata": {},
   "source": [
    "Holding all other coefficients constant, each coefficient above aligned to the order of the features suggests that for a one unit increase in the corresponding independent variable, will have the coefficient effect on the target(SalePrice) otherwise known as the dependent variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fe848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring R2-metric on Train data\n",
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf715d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring R2-metric on Test \n",
    "lr.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8647662",
   "metadata": {},
   "source": [
    "These R2 scores mean that for the Train data, the independent variables in the model explains 82.7 percent of the variability in the dependent variable price. For the Test data, the independent variables in the model explains 86.9 percent of the variability in the dependent variable price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Below root mean squared error')\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, odd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975b83e",
   "metadata": {},
   "source": [
    "While there is no such thing as a good root mean squared error as the scale of the target changes the metric, the closer to 0 it is the less error. Since the Mayor has made beating the 35,000 score his interns and volunteers achieved, this 27,417 looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469bd2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ntr=cross_val_score(lr, X_train, y_train, cv=5, n_jobs = -1) \n",
    "print(ntr)\n",
    "f'{round(mean(ntr), 2)} \\u00B1 {round(2 * stdev(ntr), 2)}'\n",
    "#I believe there are outliers and that my model is overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca97e7a",
   "metadata": {},
   "source": [
    "Aboves is the cross validation score that is less than my train score, which is in line with an expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14eddd",
   "metadata": {},
   "source": [
    "### Baseline Score is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b77741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Null Prediction---the mean\n",
    "#all R2 scores should be zero.\n",
    "#\n",
    "dr= DummyRegressor(strategy= 'mean')\n",
    "dr.fit(X_train,y_train)\n",
    "dr.predict(X_train)\n",
    "za=dr.score(X_test, y_test)\n",
    "print(za.round(decimals=1))\n",
    "\n",
    "ymeantrain=[y_train.mean() for number in range(len(X_train))]\n",
    "print(metrics.r2_score(y_train, ymeantrain))\n",
    "\n",
    "ymeantest=[y_test.mean() for number1 in range(len(X_test))]\n",
    "print(metrics.r2_score(y_test, ymeantest))\n",
    "\n",
    "yall=[y.mean() for number2 in range(len(y))]\n",
    "print(metrics.r2_score(y, yall))\n",
    "\n",
    "print('Below root mean squared error')\n",
    "print(np.sqrt(metrics.mean_squared_error(y_train, ymeantrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e0dff",
   "metadata": {},
   "source": [
    "The traditional baseline of the mean to compare this model to is listed above. Beating 0 and about 80,000 for R2 and RMSE suggest success in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698b49f",
   "metadata": {},
   "source": [
    "###  The Lasso and Ridge Models are below\n",
    "* Both regularization methods require scaling first. The code below was taken from Sophie Tabac's Intro to Regularization jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate\n",
    "ss = StandardScaler()\n",
    "# Fit and transform.\n",
    "ss.fit(X_train, y_train)\n",
    "X_train_sc = ss.transform(X_train)\n",
    "# Transform.\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a33a3",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#200 alphas with .05 steps\n",
    "l_alphas = np.arange(0.01, 10.0, 0.05)\n",
    "\n",
    "# model using lasso_coefs function from above\n",
    "l_coefs = lasso_coefs(X_train_sc, y_train, l_alphas)\n",
    "\n",
    "housing_coefs_df = pd.DataFrame(l_coefs, columns = features)\n",
    "housing_coefs_df['alpha'] = l_alphas\n",
    "for col, coef in housing_coefs_df.iloc[housing_coefs_df.index.max()]\\\n",
    "                              .iteritems():\n",
    "    if not coef:\n",
    "        print(col, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de31cf6",
   "metadata": {},
   "source": [
    "The lack of output suggests all of my variables are good, but I will make sure by checking the coefficients with the high alphas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_coefs_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1284e75",
   "metadata": {},
   "source": [
    "The method below, Lasso CV, was used to eliminate several independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit LassoCV\n",
    "lasso_cv = LassoCV(cv = 10).fit(X_train_sc, y_train)\n",
    "oddlasso=lasso_cv.predict(X_test_sc)\n",
    "\n",
    "#R2 score on Xtrain\n",
    "print('Xtrainscore:', lasso_cv.score(X_train_sc, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1142a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xtest R2\n",
    "print('Xtest R2:', lasso_cv.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Value Score\n",
    "ntx=cross_val_score(lasso_cv, X_train_sc, y_train, cv = 5, n_jobs = -1)\n",
    "\n",
    "print(ntx)\n",
    "f'{round(mean(ntx), 2)} \\u00B1 {round(2 * stdev(ntx), 2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e61f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Below is root mean squared error:')\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, oddlasso)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient values of the Model are ', lasso_cv.coef_)\n",
    "print('Intercept value is ', lasso_cv.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20881e2",
   "metadata": {},
   "source": [
    "Holding all other coefficients constant, each coefficient above aligned to the order of the features suggests that for a one unit increase in the corresponding independent variable, will have the coefficient effect on the target(SalePrice) otherwise known as the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f19070",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "Train_test_split is done from the original model. This was scaled during Lasso \n",
    "so no need to rescale. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate\n",
    "ridge_reg = RidgeCV()\n",
    "\n",
    "#Fit the model\n",
    "ridge_reg.fit(X_train_sc, y_train)\n",
    "\n",
    "#Predict\n",
    "oddridge=ridge_reg.predict(X_test_sc)\n",
    "\n",
    "# X_train_sc R2 score below\n",
    "print(ridge_reg.score(X_train_sc, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b65af5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ridge_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fe422870861a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# X_test_sc R2 score below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mridge_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ridge_reg' is not defined"
     ]
    }
   ],
   "source": [
    "# X_test_sc R2 score below\n",
    "print(ridge_reg.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntq=cross_val_score(ridge_reg, X_train_sc, y_train, cv = 5, n_jobs = -1)\n",
    "\n",
    "R2\n",
    "print(ntq)\n",
    "f'{round(mean(ntq), 2)} \\u00B1 {round(2 * stdev(ntq), 2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2233cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Below is root mean squared error:')\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, oddridge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7793650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient values of the Model are ', ridge_reg.coef_)\n",
    "print('Intercept value is ', ridge_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bdd09a",
   "metadata": {},
   "source": [
    "Holding all other coefficients constant, each coefficient above aligned to the order of the features suggests that for a one unit increase in the corresponding independent variable, will have the coefficient effect on the target(SalePrice) otherwise known as the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Submission Without Lasso\n",
    "X2=dftest[features]\n",
    "prediction=lr.predict(X2)\n",
    "\n",
    "#Kaggle Submission With Lasso\n",
    "X3=(dftest[features])\n",
    "X3sc=ss.transform(X3)\n",
    "predictionlasso=lasso_cv.predict(X3sc)\n",
    "\n",
    "#Kaggle Submission With Ridge\n",
    "#X3 works for Ridge as well.\n",
    "predictionridge=ridge_reg.predict(X3sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ffd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe\n",
    "dfpredframe=dftest\n",
    "dfpredframelasso=dftest\n",
    "dfpredframeridge=dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff004c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpredframe[['SalePrice']]=[g for g in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6fb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpredframelasso[['SalePrice']]=[h for h in predictionlasso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpredframeridge[['SalePrice']]=[k for k in predictionridge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26469b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These were used to load the csv into the data folder.\n",
    "header =['Id','SalePrice']\n",
    "#dfpredframe.to_csv('BavdaKaggle1.csv', columns=header, index=False)\n",
    "#dfpredframelasso.to_csv('BavdaKaggle1lasso.csv', columns=header, index=False)\n",
    "#dfpredframeridge.to_csv('datasets/BavdaFinal.csv', columns=header, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09155751",
   "metadata": {},
   "source": [
    "The histograms below are to check the normality of the distribution for the residuals. This is also one of the assumptions of a multivariate linear regression. The scatter plot is to check for homoscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a116d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc38f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(odd, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1300ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "residualslas = y_test-oddlasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residualslas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20642b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(oddlasso, residualslas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd52a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "residualsridge = y_test-oddridge\n",
    "plt.hist(residualsridge);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(oddridge, residualsridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a5e18",
   "metadata": {},
   "source": [
    "Checking to see if the natural log makes a difference below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a49cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.map(np.log).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da746b23",
   "metadata": {},
   "source": [
    "Transforming exponentially and logarithmically may increase accuracy and reduce RMSE. I ran into a issue when I attempted to log my dummy variables. The log of 0 breaks the model. This required separating each dataframe and logging the non dummy variables. Then, the model would be run. If I had more time, I would venture further in both. I would suggest that in the next iteration of this model, if accuracy became extraordinarily important, a natural log model would be worth looking into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804849f4",
   "metadata": {},
   "source": [
    "## [Data Visualizations](#Data-VIsualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9792c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e9b31eb",
   "metadata": {},
   "source": [
    "## [Conclusions and Recommendations](#Conclusions-and-Recommendations)\n",
    "\n",
    "As the introduction explains, the Mayor has hired Bavda Consulting to develop a model that predicts prices on home sales in Ames, Iowa. The purpose of this model is to help the Mayor identify and root out discrimination in housing. The model will serve as a benchmark as a predictor based on non-discriminatory factors such as square feet, the age of the home, the amount of Garage are, etc. This will demonstrate if traditionally disadvantaged communities are paying a premium on housing in Ames. There are two key recommendation categories that Bavda Consulting can make. First, there is a production model recommendation. Second, there is a usage recommendation. Prior to this, it should be said that there were basic assumptions of linearity between the variables along with other statistical assumption. There were also about 2,000 observations. This is a small dataset and should be supplemented with more observations to create a better predictor.\n",
    "\n",
    "### Production Model Recommendation\n",
    "\n",
    "\n",
    "The production model chosen was the Ridge Penalty Multivariate Linear Regression model. Given that the Mayor has deemed Root Mean Squared Error (RMSE) as her metric on her validation test, this model is the one that minimized RMSE on the Kaggle check. It had the least error of these models. It represents the least average distance between the model and reality. However, as a professional, I would suggest that the Ordinary Multivariate Linear Regression(OMLR) model and R2 be the ones used for the city's purposes.\n",
    "\n",
    "\n",
    "First of all, the OMLR model is the simplest model to explain. Given that the Mayor is attempting to fulfill campaign promises and has to explain her actions to the general public, simplicity may be the best choice. As is, statistical analysis is often see as magic rather than science. To attempt to explain regularization to data science students is difficult, let alone to the general public. The reality is that most people falsely believe that if they cannot understand something, then there is a problem with the substance, not their own faculties. The universe is not obligated to make sense to each of us. However, understanding the model will get buy-in from his constituents. \n",
    "\n",
    "\n",
    "Moreover, if transactions are going to be flagged as discriminatory and the model is going to be used as evidence in court and administrative hearings, the fact finder whether that be jury or judge is going to have to understand the model to find in favor of the Mayor. If not understood, the Mayor will lose. Without the deterrent of adverse legal findings based on the model, the model will become toothless.\n",
    "\n",
    "\n",
    "Furthermore, the OMLR model's metrics are not that different than the Ridge and Lasso models. The RMSE on the test data for OMLR was 27,417 while Lasso and Ridge were 27,559 and 27,444, respectively. While the Kaggle score was minimized by Ridge, this could simply be a quirk in the data-randomness at work. The Cross Validation R2 scores are nearly identical for all three models coming in at .80. This means that 80 percent of the variation in Sale Price can be explained by the features in the model. Given that there is irreducible error (some level of randomness) and discrimination may be at work to explain some of these transactions, 80 percent may be the highest this dataset will allow. R2 may also be a better metric. R2 does not depend on the scale of the dependent variable. It is always between  0 and 1, meaning it is more interpretable whereas RMSE depends on the scale.\n",
    "\n",
    "\n",
    "### Model Usage Recommendation\n",
    "\n",
    "The first recommendation would be to create an interactive version of the model on the city's website. Each buyer, advantaged or disadvantaged, could input the variables for the house they are seeking. The independent variables could be changed and played with. The garage area, total basement square feet, the overall quality rating from the assessor, the number of bathrooms, square feet of the house, whether the house was in the Northridge neighborhood, the amount of fireplaces, the finished square feet of the basement, the kitchen quality, the heating system's quality and condition, the exterior quality, the wood deck square feet, the year built, the lot area, masonry veneer area, and whether the house was brand new could all be played with. \n",
    "\n",
    "\n",
    "Each coefficient detailed in the Modeling section could be ordered in the same way as the features.  Each one unit increase in the independent variable would suggest a coefficient increase in the sale price. This would guide the community in expectations of the appropriate price and alert buyers if they sense they are being made to pay a premium regardless of advantaged status.\n",
    "\n",
    "\n",
    "Second, another more directly related to fair housing recommended use is to send in two couples, one traditionally advantaged, and the other traditionally disadvantaged, to see the price agreed upon at the new home developer's office. The model can serve as a benchmark to evaluate the discrepancy between the two prices to determine if a premium was required. The model also could be used to identify potential discrimination by entering in data from housing transactions to see if there is trend. The city would know where to use its limited resources.\n",
    "\n",
    "Third, given that there can be many uses for a model that predicts sale prices and the city has limited resources, the Mayor may well license the model to real estate agencies to earn extra revenue to fund her fight for integration. This would be a win, win for both parties.\n",
    "\n",
    "This completes our analysis, conclusion, and recommendations. Please contact Bavda Consulting with any questions, suggestions and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06979ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
